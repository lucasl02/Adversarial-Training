{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPWK4idAfZEIeEKMa1ANEzS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasl02/Adversarial-Training/blob/main/IBP_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bound-propagation\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "un3Rd-rR9LWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Model"
      ],
      "metadata": {
        "id": "AuOLW6DnDtNt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa-tOc6xiWBg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from bound_propagation import BoundModelFactory, HyperRectangle\n",
        "from tqdm import trange\n",
        "\n",
        "# from tensorboardX import SummaryWriter\n",
        "\n",
        "use_cuda = False\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "batch_size = 64\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "## Dataloaders\n",
        "train_dataset = datasets.MNIST('mnist_data/', train=True, download=True, transform=transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "))\n",
        "test_dataset = datasets.MNIST('mnist_data/', train=False, download=True, transform=transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "class FashionMNISTNetwork(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        if args:\n",
        "            # To support __get_index__ of nn.Sequential when slice indexing\n",
        "            super().__init__(*args)\n",
        "        else:\n",
        "            img_size = 28 * 28\n",
        "            classes = 10\n",
        "\n",
        "            super().__init__(\n",
        "                # nn.Flatten(),\n",
        "                nn.Linear(img_size, 50),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(50, 50),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(50, classes)\n",
        "            )\n",
        "model = FashionMNISTNetwork().to(device)\n",
        "model.train()\n",
        "factory = BoundModelFactory()\n",
        "model = factory.build(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normal Training\n",
        "\n",
        "Training the Model without IBP"
      ],
      "metadata": {
        "id": "TxjLB_GkAyzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, num_epochs):\n",
        "    print('')\n",
        "    print('[TRAINING]')\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    for epoch in trange(num_epochs):\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            images, labels = data\n",
        "            # flatten images to expected dimensions\n",
        "            images = images.view(-1, 28*28)\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.3f}')\n",
        "\n",
        "def test(net):\n",
        "    print('')\n",
        "    print('[TEST]')\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            images = images.view(-1, 28*28)\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print(f'Accuracy on images: {100 * correct / total}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UolzlcViA1vo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Executing Normal Training\n",
        "\n",
        "The clean accuracy of the model is 93.5"
      ],
      "metadata": {
        "id": "QNVPuANwBrld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUMKIPxZj0rH",
        "outputId": "90d38f1b-8abb-45b0-baa6-a96492609739"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAINING]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:07<01:06,  7.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:14<00:58,  7.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 0.592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:21<00:51,  7.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 0.412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:29<00:43,  7.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 0.354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:36<00:36,  7.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 0.322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:43<00:28,  7.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 0.301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:50<00:21,  7.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 0.284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [00:58<00:14,  7.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 0.269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:05<00:07,  7.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 0.254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:12<00:00,  7.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 0.241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmjhewsb7x3x",
        "outputId": "87c2daf0-8fad-45b6-e7d0-340d43ad858d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TEST]\n",
            "Accuracy on images: 93.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IBP Training"
      ],
      "metadata": {
        "id": "2GOsBDuxA2cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_transform():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.PILToTensor(),\n",
        "        transforms.ConvertImageDtype(torch.float),\n",
        "        transforms.Lambda(torch.flatten)\n",
        "    ])\n",
        "\n",
        "    # Identity transform - because cross entropy loss supports class indexing\n",
        "    target_transform = transforms.Compose([])\n",
        "\n",
        "    return transform, target_transform\n",
        "\n",
        "def adversarial_logit(y_hat, y):\n",
        "    batch_size = y.size(0)\n",
        "    classes = torch.arange(10, device=y.device).unsqueeze(0).expand(batch_size, -1)\n",
        "    mask = (classes == y.unsqueeze(-1)).to(dtype=y_hat.lower.dtype)\n",
        "\n",
        "    # Take upper bound for logit of all but the correct class where you take the lower bound\n",
        "    adversarial_logit = (1 - mask) * y_hat.upper + mask * y_hat.lower\n",
        "\n",
        "    return adversarial_logit\n",
        "\n",
        "\n",
        "def train_ibp(net):\n",
        "    print('')\n",
        "    print('[TRAINING]')\n",
        "\n",
        "    transform, target_transform = construct_transform()\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
        "\n",
        "    k = 1.0\n",
        "    running_eps = 0.0\n",
        "    for epoch in trange(20):\n",
        "        running_loss = 0.0\n",
        "        running_cross_entropy = 0.0\n",
        "        # e that starts from 0.0 and gradually increases to e_train = 0.1\n",
        "\n",
        "        for i, (X, y) in enumerate(train_loader):\n",
        "            X = X.view(-1, 28*28)\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            y_hat = net(X)\n",
        "\n",
        "            cross_entropy = criterion(y_hat, y)\n",
        "\n",
        "            # z_k(e_train)\n",
        "            bounds = net.ibp(HyperRectangle.from_eps(X, running_eps))\n",
        "            # z^k(e_train)\n",
        "            logit = adversarial_logit(bounds, y)\n",
        "\n",
        "            loss = k * cross_entropy + (1 - k) * criterion(logit, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            running_cross_entropy += cross_entropy.item()\n",
        "            if i % 100 == 99:  # print every 100 mini-batches\n",
        "                print(f'[{epoch + 1}, {i + 1:3d}] loss: {running_loss / 100:.3f}, cross entropy: {running_cross_entropy / 100:.3f}, k = {k}, eps = {running_eps}')\n",
        "                running_loss = 0.0\n",
        "                running_cross_entropy = 0.0\n",
        "\n",
        "        k = max(k - 0.1, 0.5)\n",
        "        # increase eps gradually to 0.1\n",
        "        running_eps = min(running_eps + (0.1/20), 0.1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_ibp(net):\n",
        "    print('')\n",
        "    print('[TEST]')\n",
        "\n",
        "    transform, target_transform = construct_transform()\n",
        "\n",
        "    correct = 0\n",
        "    for i, (X, y) in enumerate(test_loader):\n",
        "        X = X.view(-1, 28*28)\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        y_hat = net(X)\n",
        "\n",
        "        predicted = torch.argmax(y_hat, 1)\n",
        "        correct += (predicted == y).sum().item()\n",
        "\n",
        "    print(f'Accuracy: {correct / len(test_dataset):.3f}')\n"
      ],
      "metadata": {
        "id": "Z8yUWtYOjXNB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Executing IBP Training"
      ],
      "metadata": {
        "id": "50fkNCYUEjBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    net = FashionMNISTNetwork().to(device)\n",
        "\n",
        "    factory = BoundModelFactory()\n",
        "    net = factory.build(net)\n",
        "\n",
        "    train_ibp(net)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "32IkO028EkHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ibp(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJmdBWl7OlxB",
        "outputId": "10299435-c0fc-485f-eecf-6e7228bcec47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TEST]\n",
            "Accuracy: 0.935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PGD Robustness Testing"
      ],
      "metadata": {
        "id": "PxbMRbsfXTlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_linf_untargeted(model, x, labels, k, eps, eps_step):\n",
        "    model.eval()\n",
        "    ce_loss = torch.nn.CrossEntropyLoss()\n",
        "    adv_x = x.clone().detach()\n",
        "    adv_x.requires_grad_(True)\n",
        "    for _ in range(k):\n",
        "        adv_x.requires_grad_(True)\n",
        "        model.zero_grad()\n",
        "        output = model(adv_x)\n",
        "        # TODO: Calculate the loss\n",
        "        loss = ce_loss(output, labels)\n",
        "        loss.backward()\n",
        "        # TODO: compute the adv_x\n",
        "        grad = adv_x.grad\n",
        "        adv_x = adv_x  + (eps_step * torch.sign(grad))\n",
        "        # find delta, clamp with eps\n",
        "        #linf so can check values individually\n",
        "        # change has to be eps bubble at max: delta = [-eps, eps]\n",
        "        delta = torch.clamp(adv_x - x, min = -eps, max= eps)\n",
        "        # calmp to image domain: adv_x = [0,1]\n",
        "        adv_x = torch.clamp(x + delta, min = 0.0, max = 1.0).detach().requires_grad_()\n",
        "\n",
        "    return adv_x\n",
        "\n",
        "def test_model_on_single_attack(model, attack='pgd_linf', eps=0.1, k = 10):\n",
        "    model.eval()\n",
        "    tot_test, tot_acc = 0.0, 0.0\n",
        "    for batch_idx, (x_batch, y_batch) in tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Evaluating\"):\n",
        "        x_batch = x_batch.view(-1, 28*28)\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        #print('On batch: ', batch_idx)\n",
        "        if attack == 'pgd_linf':\n",
        "            # TODO: get x_adv untargeted pgd linf with eps, and eps_step=eps/4\n",
        "            adv_x = pgd_linf_untargeted(model,x_batch,y_batch,k,eps = eps,eps_step= eps/4 )\n",
        "            #\n",
        "            with torch.no_grad():\n",
        "                logits = model(adv_x)\n",
        "                predicts = logits.argmax(dim =1 )\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        # get the testing accuracy and update tot_test and tot_acc\n",
        "        '''num got right = sum(predicts = y_batch)'''\n",
        "        tot_acc += (predicts == y_batch).sum().item()\n",
        "        '''num total'''\n",
        "        tot_test += y_batch.size(0)\n",
        "        #print('Accuracy So Far: %.5lf' % (tot_acc/tot_test), f'on {attack} attack with eps = {eps}')\n",
        "\n",
        "    print('Robust accuracy %.5lf' % (tot_acc/tot_test), f'on {attack} attack with eps = {eps}')"
      ],
      "metadata": {
        "id": "nqKih_E2XVtI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normal Model"
      ],
      "metadata": {
        "id": "QBQ3eroPenFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model_on_single_attack(model=model, attack = 'pgd_linf', eps = 8/225)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJZAoETUXeF6",
        "outputId": "c1f5144b-2ed3-4dfc-810c-a8ef3abe40da"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [00:02<00:00, 59.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robust accuracy 0.80220 on pgd_linf attack with eps = 0.035555555555555556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IBP Model"
      ],
      "metadata": {
        "id": "N_GZJKlpepJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model_on_single_attack(model=net, attack = 'pgd_linf', eps = 8/225)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpseUgN7XrY6",
        "outputId": "c17469de-0f6f-4163-c19a-061d33212d72"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [00:02<00:00, 60.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robust accuracy 0.90660 on pgd_linf attack with eps = 0.035555555555555556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Box Verification"
      ],
      "metadata": {
        "id": "X2vWHDmxau85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bounds(eps):\n",
        "    input_bounds = HyperRectangle.from_eps(x, eps)\n",
        "    ibp_bounds = net.ibp(input_bounds)\n",
        "    #print(ibp_bounds.lower)\n",
        "    #print(ibp_bounds.upper)\n",
        "    return ibp_bounds.lower, ibp_bounds.upper\n",
        "\n",
        "def is_pass(lb, ub, true_labels):\n",
        "    batch_size = lb.shape[0]\n",
        "    passes = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # get logit of true class\n",
        "        true_lower = lb[i, true_labels[i]]\n",
        "        # Check all other classes\n",
        "        for class_idx in range(10):\n",
        "            if class_idx != true_labels[i]:\n",
        "                # if upper bound of another class is greater than lowber bound of true class , False\n",
        "                if ub[i, class_idx] >= true_lower:\n",
        "                    passes[i] = False\n",
        "                    break\n",
        "\n",
        "    return passes"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZkVSTlw5ZArY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilons = np.linspace(0.01, 0.1, num=10)\n",
        "for eps in epsilons:\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        #print(len(test_loader))\n",
        "        x = data.view(-1, 28*28).to(device)\n",
        "        y = target.to(device)\n",
        "        # get bounds of output neurons after passing through starting bounds\n",
        "        lb, ub = get_bounds(eps)\n",
        "\n",
        "        # get model correctness of batch\n",
        "        pass_batch = is_pass(lb, ub, y)\n",
        "        correct += pass_batch.sum().item()\n",
        "\n",
        "    print(f'Epsilon: {eps}, Accuracy: {correct/len(test_loader.dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4_NUDHRb4N1",
        "outputId": "fdeb9d7f-e927-4905-ecd7-98bc0b712ce2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsilon: 0.01, Accuracy: 0.9231\n",
            "Epsilon: 0.020000000000000004, Accuracy: 0.909\n",
            "Epsilon: 0.030000000000000006, Accuracy: 0.8941\n",
            "Epsilon: 0.04000000000000001, Accuracy: 0.873\n",
            "Epsilon: 0.05000000000000001, Accuracy: 0.8507\n",
            "Epsilon: 0.06000000000000001, Accuracy: 0.8256\n",
            "Epsilon: 0.07, Accuracy: 0.7975\n",
            "Epsilon: 0.08, Accuracy: 0.7639\n",
            "Epsilon: 0.09000000000000001, Accuracy: 0.724\n",
            "Epsilon: 0.1, Accuracy: 0.6775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G009z7FvfJgf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}